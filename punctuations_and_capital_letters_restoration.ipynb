{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CHAR LEVEL TEXT UNNORMALIZATION (CPT346 ASGN 2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foofoofoo97/punctuations_capital_letters_restoration/blob/main/punctuations_and_capital_letters_restoration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHqt5Vd4o6lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82db6b5e-88b0-4c9d-ae75-d998346b46a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuntkChnpvdp"
      },
      "source": [
        "#IMPORT LIBRARY \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time , random\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import pickle\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense,Embedding,Bidirectional,Activation, dot, concatenate,TimeDistributed\n",
        "from keras.initializers import *\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz-Q220Id7JB"
      },
      "source": [
        "#PREPROCESSING CORPUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFTlk_2j_tb7"
      },
      "source": [
        "# TOKENIZE TEXTS AT CHAR LEVEL\n",
        "def tokenize(lang):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False, num_words=None, char_level=True)\n",
        "    tokenizer.fit_on_texts(lang)\n",
        "    return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzFw1mAPpqJ0"
      },
      "source": [
        "# PREPROCESSING\n",
        "with open('/content/drive/MyDrive/346_assignment_2/2008.txt', 'r',encoding='cp1252') as f:\n",
        "    # SEPARATE SENTENCES\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "# INITIALIZATION\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "# PREPARE CORPUS\n",
        "# Input texts - list which store all lower case, punctuations removed sentences\n",
        "# Target texts - list which store unnormalized sentences\n",
        "\n",
        "#for each sentences stored in list, lines\n",
        "for line in lines:\n",
        " \n",
        "     #Convert all upper case to lower case\n",
        "     input_text = line.lower()\n",
        " \n",
        "     #Remove all characters except alphabet letters and numbers\n",
        "     input_text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", input_text)\n",
        "     \n",
        "     # added for decoding purpose\n",
        "     # \\t for START and \\n for END\n",
        "     target_text = '\\t'+ line + '\\n' \n",
        "\n",
        "     # save current input_text into list, input_texts\n",
        "     input_texts.append(input_text)\n",
        "     # save current target_text into list, target_texts\n",
        "     target_texts.append(target_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTB9c4Y2LlFa",
        "outputId": "cdde14ae-2447-47d3-db33-0954b34acba3"
      },
      "source": [
        "print(input_texts[:5])\n",
        "print(target_texts[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['chogm tangani kesan perubahan iklim ', 'kampala uganda 25 nov mesyuarat ketua ketua kerajaan komanwel chogm 2007 yang berakhir hari ini mendesak masyarakat antarabangsa melakukan sesuatu lebih konkrit menangani kesan ancaman perubahan iklim ', 'pada masa yang sama para pemimpin yang hadir mahukan negara maju membantu negara miskin mencapai matlamat pembangunan alaf baru mdg ', 'dua isu ini terkandung dalam dua kenyataan berasingan yang dikeluarkan pada akhir mesyuarat dwi tahunan selama tiga hari itu selain pernyataan bersama secara umum ', 'seramai 35 ketua kerajaan daripada 53 negara anggota komanwel menyertai chogm 2007 yang pada awalnya dibayangi isu penggantungan pakistan sehingga kelihatan menenggelam isu lain ']\n",
            "['\\tCHOGM tangani kesan perubahan iklim \\n', '\\tKAMPALA, Uganda 25 Nov. – Mesyuarat Ketua-Ketua Kerajaan Komanwel (CHOGM) 2007 yang berakhir hari ini, mendesak masyarakat antarabangsa melakukan sesuatu lebih konkrit menangani kesan ancaman perubahan iklim.\\n', '\\tPada masa yang sama, para pemimpin yang hadir mahukan negara maju membantu negara miskin mencapai matlamat pembangunan alaf baru (MDG).\\n', '\\tDua isu ini terkandung dalam dua kenyataan berasingan yang dikeluarkan pada akhir mesyuarat dwi-tahunan selama tiga hari itu selain pernyataan bersama secara umum.\\n', '\\tSeramai 35 ketua kerajaan daripada 53 negara anggota Komanwel menyertai CHOGM 2007 yang pada awalnya dibayangi isu penggantungan Pakistan sehingga kelihatan menenggelam isu lain.\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_GmYm9SeIii",
        "outputId": "23f4e6e2-42f3-45b8-ff7a-46ed56055bad"
      },
      "source": [
        "# TEXT TOKENIZATION USING KERAS PREPROCESSING TEXT\n",
        "# enc_token = save tokenized input characters\n",
        "enc_token = tokenize(input_texts)\n",
        "# reverse enc_token\n",
        "reverse_enc_dict = {i: c for c, i in enc_token.word_index.items()}\n",
        "\n",
        "# dec_token = save tokenized target characters\n",
        "dec_token = tokenize(target_texts)\n",
        "# reverse dec_token\n",
        "reverse_dec_dict = {i: c for c, i in dec_token.word_index.items()}\n",
        "\n",
        "# FOR TRAINING AND TESTING PURPOSE\n",
        "# Calculate values for number of encoder tokens, number of decoder tokens, max encoder sequence length and max decoder sequence length\n",
        "num_encoder_tokens = len(enc_token.word_index)+1  # add 1 for padding\n",
        "num_decoder_tokens = len(dec_token.word_index)+1  # add 1 for padding\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#print results\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Target Characters:')\n",
        "print(enc_token.word_index)\n",
        "print('Input Characters:')\n",
        "print(dec_token.word_index)\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 562574\n",
            "Target Characters:\n",
            "{'a': 1, ' ': 2, 'n': 3, 'e': 4, 'i': 5, 'k': 6, 'u': 7, 'r': 8, 't': 9, 'm': 10, 's': 11, 'd': 12, 'g': 13, 'l': 14, 'p': 15, 'b': 16, 'h': 17, 'y': 18, 'o': 19, 'j': 20, 'c': 21, 'w': 22, 'f': 23, '0': 24, '1': 25, '2': 26, 'z': 27, 'v': 28, '3': 29, '5': 30, '9': 31, '4': 32, '8': 33, '6': 34, '7': 35, 'q': 36, 'x': 37}\n",
            "Input Characters:\n",
            "{'a': 1, ' ': 2, 'n': 3, 'e': 4, 'i': 5, 'u': 6, 'k': 7, 'r': 8, 't': 9, 'm': 10, 'd': 11, 's': 12, 'g': 13, 'l': 14, 'p': 15, 'b': 16, 'h': 17, 'y': 18, 'o': 19, '\\t': 20, '\\n': 21, 'j': 22, '.': 23, ',': 24, 'M': 25, 'A': 26, 'S': 27, 'P': 28, 'w': 29, 'c': 30, '-': 31, 'K': 32, 'B': 33, '0': 34, 'I': 35, 'T': 36, 'f': 37, 'D': 38, 'R': 39, 'N': 40, '1': 41, '2': 42, 'U': 43, 'L': 44, 'J': 45, ')': 46, '(': 47, 'H': 48, 'z': 49, \"'\": 50, 'C': 51, 'O': 52, '3': 53, 'E': 54, '5': 55, '\"': 56, '9': 57, 'v': 58, 'F': 59, '4': 60, '8': 61, 'G': 62, '6': 63, '7': 64, 'W': 65, '“': 66, '‘': 67, 'Y': 68, '”': 69, '’': 70, 'Z': 71, ':': 72, '?': 73, '–': 74, 'V': 75, 'q': 76, 'Q': 77, 'x': 78, ';': 79, 'X': 80, '$': 81, '/': 82, '&': 83, '!': 84, '*': 85, '@': 86, '>': 87, '<': 88, '+': 89, '%': 90, '#': 91, '`': 92, '…': 93, '—': 94, '_': 95, ']': 96, '[': 97, '=': 98, 'é': 99, 'ï': 100, '\\xa0': 101, 'ñ': 102, 'â': 103, '€': 104, 'ô': 105, 'ó': 106, '»': 107, '¿': 108, '£': 109, '¦': 110, '~': 111, 'º': 112, '®': 113}\n",
            "Number of unique input tokens: 38\n",
            "Number of unique output tokens: 114\n",
            "Max sequence length for inputs: 1428\n",
            "Max sequence length for outputs: 1493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzWDl1TeIg4v",
        "outputId": "baf60a2f-4719-40cc-9dc1-7b138766b20e"
      },
      "source": [
        "# FOR LATER TESTING PURPOSE\n",
        "# SAVE IN A PICKLE FILE\n",
        "a = {'max_encoder_seq_length': max_encoder_seq_length,\n",
        "     'max_decoder_seq_length': max_decoder_seq_length,\n",
        "     'enc_vocab_size': num_encoder_tokens,\n",
        "     'dec_vocab_size': num_decoder_tokens,\n",
        "     'reverse_enc_dict': reverse_enc_dict,\n",
        "     'reverse_dec_dict': reverse_dec_dict,\n",
        "     'enc_token': enc_token,\n",
        "     'dec_token': dec_token}\n",
        "\n",
        "with open('/content/drive/MyDrive/346_assignment_2/params.pkl', 'wb') as handle:\n",
        "   pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print('SAVED IN PICKLE FILE')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAVED IN PICKLE FILE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1F6Vq1iegUk"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxZP9CWovyoQ",
        "outputId": "91c6cbe0-8efc-4804-b78f-cc2bc507614b"
      },
      "source": [
        "#READ VALIDATION INPUT DATA\n",
        "with open('/content/drive/MyDrive/346_assignment_2/norm.txt', 'r',encoding='cp1252') as f:\n",
        "\n",
        "  normalized_texts = f.read().split('\\n')\n",
        "\n",
        "print(normalized_texts[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['berkoban adalah satu bentuk ujian daripada allah s.w.t', 'kerana dalam menjalani kehidupan di dunia ini kita harus melakukan pengorbanan', 'hidup tanpa pengorbanan adalah kehidupan yang gagal', 'dalam menjalani hidup yang baik kita mesti berkorban masa dan tenaga misalnya berusaha mencari rezeki yang halal bagi mengelakkan kemiskinan', 'a merupakan tuntutan hidup kata pensyarah jabatan dakwah dan pembangunan insan akademi pengajian islam universiti malaya roslan mohamed mengenai konsep berkorban dalam islam', 'korban berasal daripada perkataan arab iaitu udhiyah', 'a menjurus kepada peristiwa pengorbanan satu keluarga iaitu nabi ibrahim dan isterinya hajar yang sanggup mengorbankan sesuatu yang amat mereka sayangi iaitu anaknya nabi ismail demi kasih dan cinta kepada allah s.w.t', 'ini adalah nilai pengorbanan yang cukup tinggi di sisi allah s.w.t', 'kerana ia bukan sahaja menguji kesabaran nabi ibrahim tetapi menduga sejauh mana ketakwaan nabi ibrahim', 'maka allah s.w.t menggantikan tempat nabi ismail dengan seekor biri-biri', 'sebagai orang islam yang beriman elakkan perasaan rugi dan sia-sia atas pengorbanan yang dilakukan kerana sesungguhnya allah s.w.t', 'akan menggantikannya dengan sesuatu yang lebih baik katanya yang merujuk kepada surah as-saffat ayat 100 hingga 111', 'dalam islam allah s.w.t menggalakkan umatnya memiliki sifat berkorban demi membantu orang yang kurang bernasib baik', 'memetik kata-kata saidina ali bin abi talib: bahawa apabila kita dapat korbankan sesuatu yang paling berharga untuk orang lain kelebihan yang akan kita peroleh nanti lebih daripada yang dikorbankan', 'oleh itu dalam melakukan sesuatu pengorbanan jangan menganggapnya sebagai sia-sia tetapi sebagai satu cara untuk mendapat peluang yang lebih baik', 'allah s.w.t menjanjikan ganjaran pahala kepada mereka yang sanggup melakukan pengorbanan dan beramal kebajikan kepada mereka yang memerlukan bantuan ujar roslan', 'mengenai konsep mengorbankan binatang pada hari-hari tasyrik secara idealnya adalah lambang kepada sebuah pengorbanan yang menuntut makna tersirat di sebalik yang tersurat', 'konsep ibadah korban lebih luas dan mampu melahirkan insan yang berwawasan serta memiliki anjakan paradigma', 'jika dikaitkan ibadah korban dengan realiti semasa hari ini kita boleh dianggap sebagai gagal sekiranya tidak dapat berkorban untuk orang lain walaupun mampu untuk mengorbankan seekor lembu atau unta', 'dalam konteks ini kita harus memperbetulkan persepsi dalam masyarakat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cgaeDcoyMpN",
        "outputId": "c8bb3934-08aa-4d21-a821-fd19fbb6488c"
      },
      "source": [
        "#READ VALIDATION TARGET DATA\n",
        "with open('/content/drive/MyDrive/346_assignment_2/unnorm.txt', 'r',encoding='cp1252') as f:\n",
        "\n",
        "  unnormalized_texts = f.read().split('\\n')\n",
        "\n",
        "print(unnormalized_texts[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\"Berkoban adalah satu bentuk ujian daripada Allah s.w.t. ', 'kerana dalam menjalani kehidupan di dunia ini, kita harus melakukan pengorbanan. ', 'Hidup tanpa pengorbanan adalah kehidupan yang gagal.', 'Dalam menjalani hidup yang baik, kita mesti berkorban masa dan tenaga misalnya berusaha mencari rezeki yang halal bagi mengelakkan kemiskinan. ', 'a merupakan tuntutan hidup,\" kata pensyarah Jabatan Dakwah dan Pembangunan Insan, Akademi Pengajian Islam, Universiti Malaya, Roslan Mohamed mengenai konsep berkorban dalam Islam.', 'Korban berasal daripada perkataan Arab iaitu udhiyah.', 'a menjurus kepada peristiwa pengorbanan satu keluarga iaitu Nabi Ibrahim dan isterinya, Hajar yang sanggup mengorbankan sesuatu yang amat mereka sayangi iaitu anaknya, Nabi Ismail demi kasih dan cinta kepada Allah s.w.t.', '\"Ini adalah nilai pengorbanan yang cukup tinggi di sisi Allah s.w.t. ', 'kerana ia bukan sahaja menguji kesabaran Nabi Ibrahim tetapi menduga sejauh mana ketakwaan Nabi Ibrahim. ', 'Maka Allah s.w.t. menggantikan tempat Nabi Ismail dengan seekor biri-biri.', '\"Sebagai orang Islam yang beriman, elakkan perasaan rugi dan sia-sia atas pengorbanan yang dilakukan kerana sesungguhnya Allah s.w.t. ', 'akan menggantikannya dengan sesuatu yang lebih baik,\" katanya yang merujuk kepada surah as-Saffat, ayat 100 hingga 111.', 'Dalam Islam, Allah s.w.t. menggalakkan umatnya memiliki sifat berkorban demi membantu orang yang kurang bernasib baik. ', 'Memetik kata-kata Saidina Ali bin Abi Talib: \"Bahawa apabila kita dapat korbankan sesuatu yang paling berharga untuk orang lain, kelebihan yang akan kita peroleh nanti lebih daripada yang dikorbankan.', '\"Oleh itu, dalam melakukan sesuatu pengorbanan, jangan menganggapnya sebagai sia-sia tetapi sebagai satu cara untuk mendapat peluang yang lebih baik. ', 'Allah s.w.t. menjanjikan ganjaran pahala kepada mereka yang sanggup melakukan pengorbanan dan beramal kebajikan kepada mereka yang memerlukan bantuan,\" ujar Roslan.', 'Mengenai konsep mengorbankan binatang pada hari-hari tasyrik, secara idealnya adalah lambang kepada sebuah pengorbanan yang menuntut makna tersirat di sebalik yang tersurat. ', 'Konsep ibadah korban lebih luas dan mampu melahirkan insan yang berwawasan serta memiliki anjakan paradigma.', '\"Jika dikaitkan ibadah korban dengan realiti semasa hari ini, kita boleh dianggap sebagai gagal sekiranya tidak dapat berkorban untuk orang lain walaupun mampu untuk mengorbankan seekor lembu atau unta.', '\"Dalam konteks ini, kita harus memperbetulkan persepsi dalam masyarakat. ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3KRa5ePoe-i"
      },
      "source": [
        "#to generate one hot sequence for input text\n",
        "#x-axis = unique character value, y-axis =number in sequence\n",
        "def get_text_encodings_gen(texts):\n",
        "\n",
        "    enc_seq = enc_token.texts_to_sequences(texts)\n",
        "    pad_seq = pad_sequences(enc_seq, maxlen=max_encoder_seq_length,\n",
        "                            padding='post')\n",
        "    pad_seq = to_categorical(pad_seq, num_classes=num_encoder_tokens)\n",
        "    return pad_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2V-ns0AoyBi"
      },
      "source": [
        "#to generate one hot sequence for target text\n",
        "def get_text_decodings_gen(texts):\n",
        "    enc_seq = dec_token.texts_to_sequences(texts)\n",
        "    pad_seq = pad_sequences(enc_seq, maxlen=max_decoder_seq_length,\n",
        "                            padding='post')\n",
        "    pad_seq = to_categorical(pad_seq, num_classes=num_decoder_tokens)\n",
        "  \n",
        "    return pad_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsjvaUtDlXD9"
      },
      "source": [
        "# GENERATE BATCH DATA\n",
        "# Generate small batch size of data to prevent RAM from crashing \n",
        "# if np.zeros capture all training data (all 562574 sentences) will cause RAM to crash\n",
        "def generate_batch(X, y, batch_size=64):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            \n",
        "            encoder_input_data = np.zeros((batch_size, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "\n",
        "            decoder_input_data = np.zeros((batch_size, max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "            decoder_target_data = np.zeros((batch_size, max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                # Each line in tensor will input one hot sequence for each encoder_input_data, decoder_input_data and decoder_target_data\n",
        "                encoder_input_data[i] = get_text_encodings_gen([input_text])\n",
        "                decoder_input_data[i] = get_text_decodings_gen([target_text])\n",
        "                decoder_target_data[i] = get_text_decodings_gen([target_text[1:]])\n",
        "\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bEuFtfpLIoW"
      },
      "source": [
        "#To build a sequence to sequence model for char level text unnormalization\n",
        "\n",
        "encoder_inputs = Input(shape=(None,num_encoder_tokens))\n",
        "encoder = Bidirectional(LSTM(128,return_sequences=True, return_state=True),merge_mode='concat')\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "\n",
        "encoder_h = concatenate([forward_h, backward_h])\n",
        "encoder_c = concatenate([forward_c, backward_c])\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens,))\n",
        "decoder_lstm = LSTM(256, return_sequences=True)\n",
        "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=[encoder_h, encoder_c])\n",
        "\n",
        "attention = dot([decoder_outputs, encoder_outputs], axes=(2, 2))\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "context = dot([attention, encoder_outputs], axes=(2, 1))\n",
        "decoder_combined_context = concatenate([context, decoder_outputs])\n",
        "\n",
        "output = TimeDistributed(Dense(128, activation=\"relu\"))(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(num_decoder_tokens, activation=\"softmax\"))(output)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], [output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bFyKxipBNv8",
        "outputId": "33d4e203-d961-4ccb-9c63-0ace731a997f"
      },
      "source": [
        "#To view model structure summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 38)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, None, 256),  171008      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None, 114)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, None, 256)    379904      input_2[0][0]                    \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, None, None)   0           lstm_1[0][0]                     \n",
            "                                                                 bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, None, None)   0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, None, 256)    0           attention[0][0]                  \n",
            "                                                                 bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, 512)    0           dot_1[0][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 128)    65664       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 114)    14706       time_distributed[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 631,282\n",
            "Trainable params: 631,282\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBMXKTlqlkIN",
        "outputId": "5eeaa03d-f030-490c-8ae6-1286d9c6a4d0"
      },
      "source": [
        "print('INPUT TEXTS LENGTH' , len(input_texts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT TEXTS LENGTH 562574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWmiipwKpsnb"
      },
      "source": [
        "# Run training\n",
        "def train_model(batch_size = 128, epochs=10):\n",
        "\n",
        "    #initialize Adam optimizer\n",
        "    tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,)\n",
        "\n",
        "    #compile model by setting up optimizer, loss function and metrics\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    #train with input_sentences and target_sentences\n",
        "    #validate with normalized_texts and unnormalized_texts\n",
        "    model.fit_generator(\n",
        "        generator=generate_batch(X=input_texts, y=target_texts, batch_size=batch_size),\n",
        "        steps_per_epoch=math.ceil(len(input_texts)/batch_size),\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        validation_data = generate_batch(X=normalized_texts, y=unnormalized_texts, batch_size=batch_size),\n",
        "        validation_steps=math.ceil(len(normalized_texts)/batch_size),\n",
        "        workers=1,\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rXgGnxszhJN",
        "outputId": "7acc45e9-1422-4f56-c6d2-aebac3de886c"
      },
      "source": [
        "#START TRAINING MODEL\n",
        "train_model(epochs=5)\n",
        "\n",
        "print('MODEL TRAINED')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "4396/4396 [==============================] - 4036s 917ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0322 - val_accuracy: 0.9566\n",
            "Epoch 2/5\n",
            "4396/4396 [==============================] - 4054s 922ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0288 - val_accuracy: 0.9947\n",
            "Epoch 3/5\n",
            "4396/4396 [==============================] - 4054s 922ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0309 - val_accuracy: 0.9950\n",
            "Epoch 4/5\n",
            "4396/4396 [==============================] - 4029s 916ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0317 - val_accuracy: 0.9925\n",
            "Epoch 5/5\n",
            "4396/4396 [==============================] - 4024s 915ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0319 - val_accuracy: 0.9931\n",
            "MODEL TRAINED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB_1wIOpMMy7",
        "outputId": "6f3b5f76-c88d-4611-e6f7-9dc6465aa96d"
      },
      "source": [
        "#SAVE TRAINED MODEL INTO TRAINING.H5\n",
        "model.save_weights('/content/drive/MyDrive/346_assignment_2/training.h5')\n",
        "\n",
        "print('MODEL SAVED')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL SAVED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzyLr7Xh4TD"
      },
      "source": [
        "#TEXT ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNf2z2fEGtq_"
      },
      "source": [
        "#to generate one hot sequence for input text\n",
        "#x-axis = unique character value, y-axis =number in sequence\n",
        "def get_text_encodings(texts, parameters):\n",
        "    enc_seq = parameters[\"enc_token\"].texts_to_sequences(texts)\n",
        "    pad_seq = pad_sequences(enc_seq, maxlen=parameters[\"max_encoder_seq_length\"],\n",
        "                            padding='post')\n",
        "    pad_seq = to_categorical(pad_seq, num_classes=parameters[\"enc_vocab_size\"])\n",
        "    return pad_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6tz-r0Yh-1e"
      },
      "source": [
        "# GET EXTRA CHARACTERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th9M1w_BGvFP"
      },
      "source": [
        "#to obtain extra characters if lower case of characters in dec_tokens not available in enc_tokens\n",
        "def get_extra_chars(parameters):\n",
        "    allowed_extras = []\n",
        "    for d_c, d_i in parameters[\"dec_token\"].word_index.items():\n",
        "        #print(d_c,d_i)\n",
        "        if d_c.lower() not in parameters[\"enc_token\"].word_index:\n",
        "            allowed_extras.append(d_i)\n",
        "    return allowed_extras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfPF2fAHjwz3"
      },
      "source": [
        "# GET MODEL INSTANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJmrs_twjve2"
      },
      "source": [
        "# GET MODEL INSTANCE\n",
        "def get_model_instance(parameters):\n",
        "    encoder_inputs = Input(shape=(None, parameters[\"enc_vocab_size\"],))\n",
        "    encoder = Bidirectional(LSTM(128, return_sequences=True, return_state=True),\n",
        "                            merge_mode='concat')\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "\n",
        "    encoder_h = concatenate([forward_h, backward_h])\n",
        "    encoder_c = concatenate([forward_c, backward_c])\n",
        "\n",
        "    decoder_inputs = Input(shape=(None, parameters[\"dec_vocab_size\"],))\n",
        "    decoder_lstm = LSTM(256, return_sequences=True)\n",
        "    decoder_outputs = decoder_lstm(decoder_inputs, initial_state=[encoder_h, encoder_c])\n",
        "\n",
        "    attention = dot([decoder_outputs, encoder_outputs], axes=(2, 2))\n",
        "    attention = Activation('softmax', name='attention')(attention)\n",
        "    context = dot([attention, encoder_outputs], axes=(2, 1))\n",
        "    decoder_combined_context = concatenate([context, decoder_outputs])\n",
        "\n",
        "    output = TimeDistributed(Dense(128, activation=\"relu\"))(decoder_combined_context)\n",
        "    output = TimeDistributed(Dense(parameters[\"dec_vocab_size\"], activation=\"softmax\"))(output)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], [output])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuMSIAyViEA0"
      },
      "source": [
        "#TEXT DECODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5CaZUmb523r"
      },
      "source": [
        "#DECODE INPUT TEXTS\n",
        "def decode(model, parameters, input_texts, allowed_extras, batch_size):\n",
        "    \n",
        "    #preprocess the input_texts\n",
        "    for sentence in input_texts:\n",
        "       #convert all case to lower case\n",
        "       sentence = sentence.lower()\n",
        "       #all characters except alphabet letters and numbers is replaced with space\n",
        "       sentence = re.sub(r\"[^a-zA-Z0-9]+\", \" \", sentence)\n",
        "       #for 2 or more consecutive space will be replaced by a single space\n",
        "       sentence=re.sub(r'[ ]{2,}',r' ',sentence) \n",
        "\n",
        "    input_texts_c = input_texts.copy()\n",
        "\n",
        "    #convert input_texts to one hot sequence\n",
        "    out_dict = {}\n",
        "    input_sequences = get_text_encodings(input_texts, parameters)\n",
        "    parameters[\"reverse_dec_dict\"][0] = \"\\n\"\n",
        "    outputs = [\"\"] * len(input_sequences)\n",
        "\n",
        "    #generate one hot sequence for decoder input data\n",
        "    target_text = \"\\t\"\n",
        "    target_seq = parameters[\"dec_token\"].texts_to_sequences([target_text] * len(input_sequences))\n",
        "    target_seq = pad_sequences(target_seq, maxlen=parameters[\"max_decoder_seq_length\"],\n",
        "                               padding=\"post\")\n",
        "    target_seq_hot = to_categorical(target_seq, num_classes=parameters[\"dec_vocab_size\"])\n",
        "\n",
        "    extra_char_count = [0] * len(input_texts)\n",
        "    prev_char_index = [0] * len(input_texts)\n",
        "    i = 0\n",
        "    while len(input_texts) != 0:\n",
        "        curr_char_index = [i - extra_char_count[j] for j in range(len(input_texts))]\n",
        "        input_encodings = np.argmax(input_sequences, axis=2)\n",
        "\n",
        "        cur_inp_list = [input_encodings[_][curr_char_index[_]] if curr_char_index[_] < len(input_texts[_]) else 0 for _\n",
        "                        in range(len(input_texts))]\n",
        "        \n",
        "        #Use trained model to predict and adjust the sequence for words\n",
        "        output_tokens = model.predict([input_sequences, target_seq_hot], batch_size=batch_size)\n",
        "        sampled_possible_indices = np.argsort(output_tokens[:, i, :])[:, ::-1].tolist()\n",
        "        sampled_token_indices = []\n",
        "        \n",
        "        #for each possible words in sentence\n",
        "        for j, per_char_list in enumerate(sampled_possible_indices):\n",
        "            for index in per_char_list:\n",
        "                # if current word is in allowed extras\n",
        "                if index in allowed_extras:\n",
        "                    if parameters[\"reverse_dec_dict\"][index] == '\\n' and cur_inp_list[j] != 0:\n",
        "                        continue\n",
        "                    elif parameters[\"reverse_dec_dict\"][index] != '\\n' and prev_char_index[j] in allowed_extras:\n",
        "                        continue\n",
        "                    sampled_token_indices.append(index)\n",
        "                    extra_char_count[j] += 1\n",
        "                    break\n",
        "                elif parameters[\"enc_token\"].word_index[parameters[\"reverse_dec_dict\"][index].lower()] == cur_inp_list[j]:\n",
        "                      sampled_token_indices.append(index)\n",
        "                      break\n",
        "\n",
        "        sampled_chars = [parameters[\"reverse_dec_dict\"][index] for index in sampled_token_indices]\n",
        "\n",
        "        outputs = [outputs[j] + sampled_chars[j] for j, output in enumerate(outputs)]\n",
        "        end_indices = sorted([index for index, char in enumerate(sampled_chars) if char == '\\n'], reverse=True)\n",
        "        for index in end_indices:\n",
        "            out_dict[input_texts[index]] = outputs[index].strip()\n",
        "            del outputs[index]\n",
        "            del input_texts[index]\n",
        "            del extra_char_count[index]\n",
        "            del sampled_token_indices[index]\n",
        "            input_sequences = np.delete(input_sequences, index, axis=0)\n",
        "            target_seq = np.delete(target_seq, index, axis=0)\n",
        "        if i == parameters[\"max_decoder_seq_length\"] - 1 or len(input_texts) == 0:\n",
        "            break\n",
        "        target_seq[:, i + 1] = sampled_token_indices\n",
        "        target_seq_hot = to_categorical(target_seq, num_classes=parameters[\"dec_vocab_size\"])\n",
        "        prev_char_index = sampled_token_indices\n",
        "        i += 1\n",
        "    outputs = [out_dict[text] for text in input_texts_c]\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxOwVu-jicSw"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BozJqxRCwB4E"
      },
      "source": [
        "class Testing():\n",
        "    def __init__(self):\n",
        "        \n",
        "        # LOAD PARAMS\n",
        "        with open('/content/drive/MyDrive/346_assignment_2/params.pkl', \"rb\") as file:\n",
        "            self.parameters = pickle.load(file)\n",
        "        self.parameters[\"reverse_enc_dict\"] = {i: c for c, i in self.parameters[\"enc_token\"].word_index.items()}\n",
        "\n",
        "        # LOAD MODEL\n",
        "        self.model = get_model_instance(self.parameters)\n",
        "        self.model.load_weights('/content/drive/MyDrive/346_assignment_2/training.h5')\n",
        "\n",
        "        #GET EXTRA CHARACTERS\n",
        "        self.allowed_extras = get_extra_chars(self.parameters)\n",
        "\n",
        "    def new_test(self, batch_size=128):\n",
        "        \n",
        "        #READ NORM.TXT AND SPLIT SENTENCES INTO A LIST, NORMALIZED_TEXTS\n",
        "        with open('/content/drive/MyDrive/346_assignment_2/norm.txt', 'r',encoding='cp1252') as f:\n",
        "             normalized_texts = f.read().split('\\n')\n",
        "\n",
        "        #DECODE NORMALIZED_TEXTS\n",
        "        results =decode(self.model, self.parameters,normalized_texts[:100], self.allowed_extras, batch_size)\n",
        "        \n",
        "        print(\"Test result is also saved in test_result.txt\")\n",
        "        print(\"__________________________________________ CHAR LEVEL TEXT UNNORMALIZATION _________________________________________\")\n",
        "        print(\"__________________________________________________ GENERATED RESULT ________________________________________________\\n\")\n",
        "        \n",
        "        #WRITE TO TEXT FILE\n",
        "        file = '/content/drive/MyDrive/346_assignment_2/test_result.txt'\n",
        "        with open(file, 'w') as write_result:\n",
        "            index =0\n",
        "            #for each sentences stored in results list\n",
        "            for sentence in results:\n",
        "                #write sentence into test_result file + next line '\\n'\n",
        "                write_result.write(sentence+'\\n')\n",
        "                \n",
        "                index=index+1\n",
        "\n",
        "                #print current index and sentence\n",
        "                print(str(index)+' '+sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy9IhUB3wIZr",
        "outputId": "069dcc78-1d43-4cef-8ca2-8cf33ae3529c"
      },
      "source": [
        "#CALL FUNCTION\n",
        "#START TESTING\n",
        "Testing().new_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test result is also saved in test_result.txt\n",
            "__________________________________________ CHAR LEVEL TEXT UNNORMALIZATION _________________________________________\n",
            "__________________________________________________ GENERATED RESULT ________________________________________________\n",
            "\n",
            "1 Berkoban adalah satu bentuk ujian daripada Allah SWT\n",
            "2 Kerana dalam menjalani kehidupan di dunia ini kita harus melakukan pengorbanan\n",
            "3 Hidup tanpa pengorbanan adalah kehidupan yang gagal\n",
            "4 Dalam menjalani hidup yang baik, kita mesti berkorban masa dan tenaga misalnya berusaha mencari rezeki yang halal bagi mengelakkan kemiskinan\n",
            "5 A merupakan tuntutan hidup, kata Pensyarah Jabatan Dakwah dan Pembangunan Insan Akademi Pengajian Islam Universiti Malaya, Roslan Mohamed mengenai konsep berkorban dalam Islam\n",
            "6 Korban berasal daripada perkataan Arab iaitu Udhiyah\n",
            "7 A menjurus kepada peristiwa pengorbanan satu keluarga iaitu Nabi Ibrahim dan isterinya hajar yang sanggup mengorbankan sesuatu yang amat mereka sayangi iaitu anaknya Nabi Ismail demi kasih dan cinta kepada Allah SWT\n",
            "8 Ini adalah nilai pengorbanan yang cukup tinggi di sisi Allah SWT\n",
            "9 Kerana ia bukan sahaja menguji kesabaran Nabi Ibrahim tetapi menduga sejauh mana ketakwaan Nabi Ibrahim\n",
            "10 Maka Allah SWT menggantikan tempat Nabi Ismail dengan seekor biribiri\n",
            "11 Sebagai orang Islam yang beriman, Elakkan perasaan rugi dan siasia atas pengorbanan yang dilakukan kerana sesungguhnya Allah SWT\n",
            "12 Akan menggantikannya dengan sesuatu yang lebih baik, katanya yang merujuk kepada surah Assaffat ayat 100 hingga 111\n",
            "13 Dalam Islam Allah SWT menggalakkan umatnya memiliki sifat berkorban demi membantu orang yang kurang bernasib baik\n",
            "14 Memetik katakata, Saidina Ali bin Abi Talib bahawa apabila kita dapat korbankan sesuatu yang paling berharga untuk orang lain kelebihan yang akan kita peroleh nanti lebih daripada yang dikorbankan\n",
            "15 Oleh itu dalam melakukan sesuatu pengorbanan jangan menganggapnya sebagai siasia tetapi sebagai satu cara untuk mendapat peluang yang lebih baik\n",
            "16 Allah SWT menjanjikan ganjaran pahala kepada mereka yang sanggup melakukan pengorbanan dan beramal kebajikan kepada mereka yang memerlukan bantuan, ujar Roslan\n",
            "17 Mengenai konsep mengorbankan binatang pada Harihari tasyrik secara idealnya adalah lambang kepada sebuah pengorbanan yang menuntut makna tersirat di sebalik yang tersurat\n",
            "18 Konsep ibadah korban lebih luas dan mampu melahirkan insan yang berwawasan serta memiliki anjakan paradigma\n",
            "19 Jika dikaitkan ibadah korban dengan realiti semasa hari ini, kita boleh dianggap sebagai gagal sekiranya tidak dapat berkorban untuk orang lain walaupun mampu untuk mengorbankan seekor lembu atau unta\n",
            "20 Dalam konteks ini kita harus memperbetulkan persepsi dalam masyarakat\n",
            "21 Ramai yang mampu mengorbankan binatang tetapi dalam erti kata sebenar tidak dapat menjiwai pengertian korban, kata Roslan mengaitkan ibadah korban dengan pengertian pengorbanan\n",
            "22 Untuk menyemai semangat pengorbanan yang semakin hilang penghayatannya, beliau berkata, ibu bapa boleh menterjemahkannya dalam bentuk amalan harian dengan menunjukkan teladan yang baik kepada anakanak\n",
            "23 Secara tidak langsung tindakan ini akan melahirkan anakanak yang mudah membantu orang lain yang kurang bernasib baik\n",
            "24 Kita perlu lahirkan keluarga yang ada nilainilai pengorbanan kerana pada hari ini masyarakat Islam khususnya melalui Transformasi Dunia yang penuh cabaran\n",
            "25 Sudah sampai satu tahap umat Islam jangan hanya sekadar membaca sejarah dan kisah mengenai Nabi Ibrahim sebaliknya melihat erti pengorbanan dalam konteks yang lebih luas, katanya\n",
            "26 Senario menarik yang turut berkait dalam isu pengorbanan ialah penglibatan ramai remaja Islam dengan gejala sosial seperti mat rempit menghisap dadah pelacuran dan sebagainya\n",
            "27 Bagi membanteras permasalahan ini, banyak pihak perlu tampil untuk menyelamatkan remaja yang telah terkorban jiwa dan akhlak mereka akibat terpengaruh dengan rentak dunia yang semakin mencabar\n",
            "28 Tegas beliau, erti pengorbanan masih belum diungkap sepenuhnya kerana persepsi masyarakat yang mengaitkannya dengan pemikiran yang sempit\n",
            "29 Pengorbanan yang jelas dapat diungkap apabila anakanak berkorban menjaga ibu bapa yang sakit suami menjaga isteri yang sakit isteri menjaga suami yang sakit atau ada yang mengorbankan kehidupan mereka demi memberi nafas baru kepada golongan kurang bernasib baik\n",
            "30 Mengenai umat Islam luar negara yang ditindas seperti mana yang berlaku di negaranegara Barat, Roslan berkata, pengorbanan yang boleh dilakukan untuk membantu mereka ialah sentiasa berdoa kepada Allah SWT\n",
            "31 Memohon kesejahteraan mereka selain menghulurkan bantuan dari segi tenaga dan kewangan\n",
            "32 Dalam situasi begini juga, umat Islam perlu memiliki sifat hiba sedih dan marah apabila melihat saudara seislam ditindas dan hak mereka diketepikan\n",
            "33 Semangat berkorban perlu dijadikan satu budaya agar ia sebati dalam kehidupan seluruh umat manusia\n",
            "34 Situasi ini akan menjana keutuhan masyarakat secara keseluruhannya sebagaimana sabda Rasulullah SAW\n",
            "35 yang bermaksud: Sesiapa yang tidak mempedulikan orang Islam yang lain, maka dia bukan dari kalangan orang Islam\n",
            "36 Dalam erti kata lain, budaya berkorban yang diterapkan dalam masyarakat mampu menghindarkan sifat dendam dengki khianat dan perbalahan, sebaliknya membina umat Islam yang ampuh dan berkualiti\n",
            "37 Dalam Islam pengorbanan ialah melakukan apa yang mampu kita lakukan termasuk korbankan masa wang ringgit dan tenaga untuk membantu orang lain yang dalam kesusahan\n",
            "38 Contohnya dalam Korban Binatang Ada Usaha Rakyat Malaysia yang melakukannya di Kemboja, Vietnam dan Thailand, jelas Roslan\n",
            "39 Menurut beliau, seseorang yang tidak pernah berkorban untuk orang lain sebenarnya tidak menghargai kehidupan yang dilaluinya di dunia\n",
            "40 Pepatah ada menyebut buat baik berpadapada buat jahat jangan sekali\n",
            "41 Apabila seseorang membantu orang lain yang dalam kesusahan bantuan itu akan diingat dan dikenang hingga ke akhir hayat\n",
            "42 Secara tidak langsung ia membawa kepada keakraban dalam kalangan umat Islam\n",
            "43 Mohd. Daniel Abdul Rahman (dua dari kiri) dan rakanrakan bersama Ketua Pegawai Eksekutif Celcom, Datuk Seri Shazalli Ramly (tiga dari kanan) pada Seminar Persatuan Pemasaran di UiTM, Shah Alam barubaru ini\n",
            "44 Setiap impian atau Citacita yang ingin dicapai harus dimulai dengan perancangan memandangkan minatnya dalam bidang perniagaan, Mohd. Daniel Abdul Rahman mengagumi Tan Sri Syed Mokhtar Albukhari\n",
            "45 Daniel menjadikan tokoh korporat itu sebagai idola yang tidak kedekut berkongsi kekayaan, beliau dengan masyarakat dan membantu selain meningkatkan martabat agama Islam dan organisasi bumiputera\n",
            "46 Contohnya dengan tertubuhnya Yayasan Albukhari\n",
            "47 Sifat ketokohan itu menjadi pegangan dan prinsip pelajar tahun akhir Universiti Teknologi Mara (UiTM) Shah Alam dalam Kursus Sarjana Muda Kepujian Pemasaran Ini\n",
            "48 Pada tahun hadapan Daniel bakal menjalani latihan praktikal di Naza Kia Sdn. Bhd\n",
            "49 Selain mengejar impiannya itu, beliau juga masih belajar untuk mendapatkan keputusan yang baik dalam akademik\n",
            "50 Sebenarnya apabila bergelar pelajar atau mahasiswa, anggapan dan harapan setiap mahasiswa di Malaysia untuk membawa satu perubahan pada diri keluarga agama, bangsa dan negara tercinta\n",
            "51 Mahasiswa atau pelajar perlu bijak dalam menjalani kehidupan di Universiti Yang Jauh daripada perhatian keluarga\n",
            "52 Ini supaya mereka tidak terpengaruh dan melibatkan diri dengan kegiatan yang tidak berfaedah\n",
            "53 Seharusnya peluang berada di Institusi Pengajian Tinggi Awam (IPTA) digunakan untuk memanfaatkan masa hadapan\n",
            "54 Mahasiswa harus berfikir bagaimana mengelakkan diri daripada digelar penganggur atau graduan yang tidak mempunyai kemahiran komunikasi yang baik, kurang penampilan dan keyakinan diri serta penguasaan bahasa Inggeris selain bahasa Melayu harus dititikberatkan\n",
            "55 Tiada gunanya kalau kita tamat pengajian dengan cemerlang tetapi lemah dalam komunikasi apatah lagi mempunyai kurang kemahiran kepimpinan diri\n",
            "56 Mungkin ini merupakan jawapan kepada masalah pengangguran yang agak serius, katanya kepada Utusan Malaysia barubaru ini\n",
            "57 Ditanya mengenai perkembangan generasi hari ini, Daniel berkata, mereka harus sedar dan serius dalam menentukan masa depan agama, bangsa dan negara\n",
            "58 Jangan jadikan alasan masa muda hanya sekali untuk berseronok dan terjebak dalam kegiatan yang tidak berfaedah sehingga melampau batas\n",
            "59 Ini akhirnya menggelapkan masa depan mereka sendiri\n",
            "60 Anakanak muda masa ini harus sedar merekalah generasi yang akan mewarisi kepimpinan masa hadapan\n",
            "61 Pelbagai masalah terutamanya masalah sosial seperti perlumbaan haram pergaulan bebas najis dadah dan seumpamanya sedang serius melanda generasi muda Melayu sekarang\n",
            "62 Tindakan menyerahkan bulatbulat kepada badanbadan kerajaan dan pertubuhanpertubuhan bukan kerajaan sematamata untuk mengatasi masalah ini adalah satu kesilapan\n",
            "63 Harus diingat bahawa asasasas atau pegangan yang kukuh terhadap nilai agama Islam sebenarnya boleh mencegah perkara tersebut, jelas anak muda ini lagi\n",
            "64 Sementara itu, tidak kurang juga kalangan anakanak muda yang cemerlang dalam bidang masingmasing yang boleh dibanggakan\n",
            "65 KEJAYAAN Najmil Faiz Mohamed Aris (pelajar Malaysia) yang mencatat sejarah membentangkan hasil kajiannya mengenai teknologi Nano di Parlimen Britain dan pelbagai adalah contoh anak muda yang tidak menghampakan bangsanya\n",
            "66 Menurut Daniel Mahasiswa dan belia cemerlang boleh dikategorikan dalam berbagaibagai bidang sama ada pendidikan politik, kerjaya Sukan dan sebagainya\n",
            "67 Belia yang cemerlang pada asasnya adalah belia yang berpegang teguh dan mengamalkan nilainilai Murni Islam\n",
            "68 Belia sewajarnya mengambil kisah dan tidak memandang enteng setiap yang berlaku\n",
            "69 Suarakan segala apa yang dipersetujui ataupun sebaliknya\n",
            "70 Jadilah belia yang boleh memberi penyelesaian terhadap masalah bangsanya\n",
            "71 Dan mengejar Citacita setinggi mana yang dikehendaki tetapi jangan lupa untuk pandang ke bawah sekiranya bantuan kita diperlukan oleh masyarakat, katanya\n",
            "72 Bekas Yang Dipertua Majlis Perwakilan Pelajar UiTM Perlis tahun 2003 menambah di kampus beliau aktif pada peringkat pelajar dan masyarakat\n",
            "73 Selain berkesempatan bermesyuarat dan bertemu dengan pemimpin masyarakat serta tokohtokoh korporat adalah pengalaman yang amat bermakna\n",
            "74 Daniel turut pernah memegang jawatan Ahli Majlis Mesyuarat Perundingan Pelajar Pulau Pinang 2004, Presiden Kebajikan Pemasaran UiTM (SHAH) alam 2005/ 2006, Biro Keusahawanan dan Alumni Majlis Perwakilan Pelajar (UITM) 2005/ 2006\n",
            "75 Anak kelahiran Pulau Pinang ini turut berasa terpanggil untuk memberi pandangan berkaitan isu Melayu pulau itu yang hangat diperkatakan barubaru ini\n",
            "76 Masyarakat Melayu Pulau Pinang harus sedar kedudukan mereka sama ada dari segi ekonomi ataupun Sosial\n",
            "77 Perubahan harus dimulai oleh setiap jiwa orang Melayu\n",
            "78 Dengan kata lain, penguasaan agama dan ilmu itu penting kerana hal yang sedemikian akan memartabatkan bangsa dan mengelakkan daripada bangsa kita dihina, tegasnya\n",
            "79 Pada masa ini Melayu Pulau Pinang amat memerlukan pemimpin yang benarbenar Ikhlas memperjuangkan nasib bangsanya dan bukannya perjuangan yang bertunjangkan pangkat harta mahupun wang ringgit\n",
            "80 Kita bertuah kerana mempunyai Perdana Menteri, Datuk Seri Abdullah Ahmad Badawi yang merupakan anak jati Melayu Pulau Pinang dan beliau amat serius dalam menyelesaikan isu ini\n",
            "81 Tetapi kita harus sedar Perdana Menteri bertanggungjawab kepada 26 juta rakyat Malaysia\n",
            "82 Justeru, pemimpin di semua negeri harus mempunyai agenda yang lebih khusus dan serius dalam mengangkat martabat bangsa Melayu, kata Daniel mengakhiri perbualan\n",
            "83 Syeikh Wan Senik Karisiqi Moyang Syeikh Daud Alfathani (\n",
            "84 Menyambut tahun baru 2007, Artikel yang dipaparkan ini adalah merupakan keluaran pertama tentang ulama Nusantara dalam bahagian agama Utusan Malaysia\n",
            "85 Atau merupakan artikel yang ke147 jika dihitung mulai siaran tahun 2004 sepanjang tahun 2005 sehingga Disember 2006\n",
            "86 Sekali gus pula kita menyambut pemasyhuran Sultan Mizan Zainal Abidin Almarhum Sultan Mahmud Almuktafi Billah Shah sebagai Yang Dipertuan Agong Malaysia yang ke13 pada 13 Disember 2006\n",
            "87 Baginda berasal dari kerajaan Terengganu\n",
            "88 Sepanjang saya memperkenalkan Ulamaulama Dunia Melayu dalam tahuntahun yang tersebut ada beberapa orang ulama Terengganu juga telah diperkenalkan termasuklah seorang Sultan Terengganu yang Alim dan Warak\n",
            "89 Baginda ialah Sultan Zainal Abidin III\n",
            "90 Hingga kini tiada siapa dapat menafikan bahawa yang mencorakkan Islam di Terengganu pertama ialah dari keturunan Saiyid dan yang kedua ialah Ulamaulama yang berasal dari Patani\n",
            "91 Dalam keluaran ini bagi saya adalah merupakan sambutan tahun baru 2007, sekali gus menyambut adanya jati diri keperkasaan dan kemahawangsaan Melayu Islam dibuktikan daulat Rajaraja Melayu dilambangkan dengan pemasyhuran Yang Dipertuan Agong pada tarikh tersebut di atas\n",
            "92 Artikel ini mengkhususkan perkara salasilah ulama Terengganu yang sangat erat dengan ulama Patani\n",
            "93 Semua ulama yang tersebut ada hubungan yang sangat erat selain kaitan sanad ilmu sekali gus di antara mereka ada yang erat hubungan nasab\n",
            "94 Syeikh Wan Senik Alkarisiqi iaitu Moyang Syeikh Daud bin Abdullah Alfathani yang bererti juga Syeikh Wan Senik adalah termasuk moyang atau datuk nenek kepada beberapa ulama Terengganu yang terdekat di antara mereka seperti Syeikh Abdul Qadir Tok Bukit Bayas, Syeikh Abdullah bin Muhammad Amin Tok Duyung dan Lainlain\n",
            "95 Jauh sebelum penghijrahan Syeikh Daud bin Abdullah Alfathani dan rombongan ke Pulau Duyung Kecil Terengganu, Jalinan Kekeluargaan Patani dan Terengganu berjalan sudah sejak lama sekurangkurangnya sejak Sultan Zainal Abidin Sultan Terengganu (lagi\n",
            "96 Hingga sekarang hubungan tersebut tidak pernah terputus\n",
            "97 Walaupun ada beberapa pertikaian pendapat di antara satu versi salasilah dengan salasilah versi yang lainnya namun tidak bererti memutuskan hubungan kekeluargaan\n",
            "98 Bahkan dengan adanya pertikaian pendapat yang termaktub dalam salasilah adalah mendorong kita supaya melakukan penyelidikan yang lebih kemas jika ada tenaga dan berkemampuan berfikir tetap berusaha untuk mentahqiqkannya\n",
            "99 Dalam artikel yang lalu disebutkan bahawa Wan Husein Assanawi mempunyai ramai adikberadik\n",
            "100 Antaranya ialah Syeikh Wan Senik Alkarisiqi\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}